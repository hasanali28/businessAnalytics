{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "m9vaW928aTeU",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Introduction to Pandas\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. In [Numpy Is Not Enough](#problem), you will learn why Numpy alone is not sufficient for data analysis\n",
    "<br><br>\n",
    "2. [Pandas: An Overview](#overview) will introduce you to Pandas and give you a brief overview of the functionality the library offers\n",
    "<br><br>\n",
    "3. [Introducing Pandas Objects](#objects) will introduce you to the Pandas data structures you will use to explore and manipulate data\n",
    "<br><br>\n",
    "4. In [Creating Your Own Series & DataFrames](#create), you will learn how to create your own Pandas Series and DataFrames from scratch and investigate their stucture using certain attributes\n",
    "<br><br>\n",
    "5. [Reading CSVs & Excels as Pandas DataFrames](#read) will teach you how to circumvent creation from scratch and load in your work files from disk or the web\n",
    "<br><br>\n",
    "6. [Preliminary Investigation of a Dataset](#prelim) will teach you the basic first steps that you will need to invoke when exploring any dataset\n",
    "<br><br>\n",
    "7. In [Selecting single and multiple columns](#select), you will learn how to select single and multiple columns from a DataFrame\n",
    "<br><br>\n",
    "8. [Indexing, Selecting and Filtering](#indexing) will build on the previous chapter and teach you how to index and filter rows and columns \n",
    "<br><br>\n",
    "9. In [Sorting Your Data](#sorting), you will learn how to sort your data by single/multiple columns, in ascending/descending order etc. You will also learn about the inplace argument, which lets you modify the original dataframe\n",
    "<br><br>\n",
    "10. [Dropping & Renaming Columns](#drop_rename) will teach you how to drop irrelevant columns and rename weirdly named columns in your data\n",
    "<br><br>\n",
    "11. In [Updating Values & Creating New Columns](#update), you will learn how to update values in your data and create new columns, which are dependent on current column values\n",
    "<br><br>\n",
    "12. [Aggregating Values](#agg) will teach you the basics of grouping and aggregating values in your data. You will also learn how to generate Excel-style pivot tables from your data\n",
    "<br><br>\n",
    "13. Although [Merging Dataframes](#merge) is not part of the exercises, it will teach you the different ways to combine and join multiple dataframes — a critical skill for when you work on your use case and daily projects\n",
    "<br><br>\n",
    "14. Lastly, in [Saving Files](#save), you will learn how to export your dataframes as CSV, Excel files etc.\n",
    "<br><br>\n",
    "\n",
    "The more refined & detailed list of learning outcomes can be found in the [Summary of Learning Outcomes](#summary) section at the end of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"problem\"></a>\n",
    "\n",
    "## Numpy Is Not Enough\n",
    "\n",
    "- Numpy is the backbone for scientific computing in Python due to its performance advantages\n",
    "- However, it still leaves some functionality to be desired:\n",
    "    - Intuitively and conveniently attach row & column labels to 2-D arrays (eg spreadsheets)\n",
    "    - Native ability to handle missing values — a rather common data problem\n",
    "    - Load data from a wide variety of sources (eg CSV, Excel, SQL etc.)\n",
    "    - Transformations that do not nicely translate to element-wise operations like pivot tables, grouping etc\n",
    "    - Intuitively and conveniently handle heterogenous values (e.g. Col A → PC9 (`str`), Col B → Price (`int`) etc)\n",
    "    - Built-in data visualization capabilities (we will cover the dataviz libraries in the next notebook)\n",
    "- Welcome to the world of Pandas — the library builds on Numpy and `ndarray`s to solve many of these problems\n",
    "    - Experienced with Excel? You'll feel much more at home with Pandas\n",
    "\n",
    "<figure>\n",
    "  <img src='img/numpy_excel-to-pandas.png' style='width:800px'/>\n",
    "  <figcaption>Image from <a href='https://jalammar.github.io/visual-numpy/'>A Visual Intro to NumPy and Data Representation</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "\n",
    "## Pandas: An Overview\n",
    "\n",
    "<figure>\n",
    "  <img src='img/pandas_logo.png' style='width:500px'/>\n",
    "</figure> \n",
    "\n",
    "Here are just a few of the things that pandas does well (taken from the official Pandas documentation):\n",
    "\n",
    "- Human-readable **row & column labelling** (just like Excel spreadsheets!)\n",
    "    - Augmented by intelligent **label-based slicing**, **fancy indexing**, and **subsetting** of large data sets\n",
    "- Easy handling of **missing data** (represented as NaN) in floating point as well as non-floating point data\n",
    "- Flexible **reshaping** and **pivoting** of data sets\n",
    "- Intuitive **merging** and **joining** of multiple data sets, based on keys\n",
    "- Powerful, flexible **group by** to perform _split-apply-combine_ operations for aggregating and transforming data \n",
    "- Size mutability: columns can be **inserted and deleted** from DataFrame and higher dimensional objects\n",
    "- Make it **easy to convert** ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects\n",
    "- **Hierarchical labeling** of axes (possible to have multiple labels per row or column)\n",
    "- **Robust IO tools** for loading data from flat files (CSV and delimited), Excel files, databases, and saving / loading data from the ultrafast HDF5 format\n",
    "- **Time series**-specific functionality: date range generation and frequency conversion, moving window statistics, moving window linear regressions, date shifting and lagging, etc.\n",
    "\n",
    "Needless to say, there is a lot of ground to cover → we cover the top functionality that will give you a solid start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't need to import numpy, but it's a common enough library that you might as well import it whenever you import pandas\n",
    "import numpy as np\n",
    "import pandas as pd # Standard practice to import as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 564,
     "status": "ok",
     "timestamp": 1525333159054,
     "user": {
      "displayName": "aycacanli@hotmail.com",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "106415820899870547758"
     },
     "user_tz": -180
    },
    "id": "-Y0upIU4aTej",
    "outputId": "6a3f1bd7-789f-4280-a71d-3eae8ca40b62",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 0.25.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Pandas version:\",pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"objects\"></a>\n",
    "\n",
    "## Introducing Pandas Objects\n",
    "\n",
    "- They can be thought of as enhanced versions of NumPy structured arrays\n",
    "    - Rows and columns are identified with _labels_ rather than simple integer indices\n",
    "- There are 3 fundamental Pandas data structures\n",
    "    - **Series**: Think of this as a single column in an Excel spreadsheet\n",
    "    - **DataFrame**: Think of this as a spreadsheet in an Excel workbook\n",
    "    - **Index**: Think of this as the array of row/column labels that make identification possible\n",
    "- In this notebook, we'll focus mostly on getting you comfortable with Series & Dataframes\n",
    "   - Pandas Index is a slightly trickier customer - resources will be provided at the end for those interested!\n",
    "\n",
    "<figure>\n",
    "  <img src='img/pandas-series_df_index.png' style='width:500px'/>\n",
    "  <figcaption><a href='https://bookdata.readthedocs.io/en/latest/base/01_pandas.html'>Image from here</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "The entire structure is the Dataframe, composed of multiple Series and Indeces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create\"></a>\n",
    "\n",
    "## Skill #1: Creating Your Own Series & DataFrames\n",
    "\n",
    "- Although you will mostly be reading in your own txt/csv/excel files to convert them into Series & Dataframes, it pays to know how to manually build series & dataframes from scratch\n",
    "- There are 2 data structures we have studied so far, which are closest to Series & Dataframes\n",
    "    - **Numpy arrays**\n",
    "    - **Dictionaries**\n",
    "- Pandas objects can be conceived to be _generalization_ of `ndarrays`\n",
    "    - Specifically, `ndarray`s have _implicitly defined integer indeces_ that let us access their values \n",
    "        - E.g., think of how we use `arr1d[0]` to access the 1st element of `arr1d`\n",
    "        - Quite restrictive → forced to use the implicit integer index (`0, 1, 2...`) to access elements\n",
    "    - `Series` → access elements using _flexible, explicitly defined row index_\n",
    "    - `DataFrame` → access elements using _flexible, explicitly defined row & column index_\n",
    "- Pandas objects can be thought of as _specialization_ of `dict`s\n",
    "    - Generally, `dict` maps arbitrary keys to a set of arbitrary values        \n",
    "    - `Series` → maps typed keys to typed values\n",
    "        - Same `dict` can have `int`, `float`, `str`, `tuple` etc keys\n",
    "        - In the same `Series`, however, all the keys should be of the same type. Typing allows efficient operations\n",
    "        - The collection of all keys is the `Index` of the `Series`\n",
    "        - Just as \"typing\" makes Numpy arrays more efficient than Python lists for many ops, Pandas Series are more efficient than dictionaries\n",
    "    - `Dataframe` → maps a column name to a `Series`\n",
    "        - The collection of all column names is the Column `Index` of the Dataframe (as we saw earlier, there are 2 `Index`-es for the same object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Your Own Series\n",
    "\n",
    "- We can build a `Series` using `pd.Series(data)`\n",
    "- `data` can be a list, numpy array, scalar, dict, or another `Series`\n",
    "- Once created, we can invoke the following attributes to understand the structure:\n",
    "    - `values` → underlying numpy array that contains the values\n",
    "    - `shape` → shape of the Pandas Series (specifically, shape of underlying `ndarray`)\n",
    "    - `index` → `Index` associated with the `Series`\n",
    "    - `dtype` → the fixed `dtype` of the values in the `Series`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series created from a list/n:{ds_list}\n"
     ]
    }
   ],
   "source": [
    "# Building a Series from a list\n",
    "ds_list = pd.Series([1, 2, 3, 4, 5])\n",
    "print('Series created from a list/n:{ds_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series with a different index/n:{ds_list_idx}\n"
     ]
    }
   ],
   "source": [
    "# pd.Series has index parameter that lets you specify an explicitly different index\n",
    "emp_names = ['John Snow', 'Tyrion Lannister', 'Bilbo Baggins', 'Frodo Baggins','Zaphod Beeblebrox']\n",
    "salaries = [0, 25000, 30000, 40000, 50000]\n",
    "ds_list_idx = pd.Series(salaries, index=emp_names)\n",
    "print('Series with a different index/n:{ds_list_idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -0.460405\n",
       "1    0.465941\n",
       "2    0.380658\n",
       "3   -0.524652\n",
       "4    1.421018\n",
       "5   -0.069607\n",
       "6    0.111981\n",
       "7   -0.009072\n",
       "8   -1.148575\n",
       "9   -0.318249\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a Series from a numpy array\n",
    "np_array = np.random.randn(10)\n",
    "ds_arr = pd.Series(np_array)\n",
    "ds_arr\n",
    "\n",
    "# Why does this display the contents w/out print? Why is it not showing any error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    Some value\n",
       "4    Some value\n",
       "5    Some value\n",
       "6    Some value\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a Series from a single scalar value\n",
    "ds_scalar = pd.Series('Some value', index=[2, 4, 5, 6]) # Manually setting specific index\n",
    "ds_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "California    38332521\n",
       "Texas         26448193\n",
       "New York      19651127\n",
       "Florida       19552860\n",
       "Illinois      12882135\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a Series from a dictionary\n",
    "population_dict = {'California': 38332521,\n",
    "                   'Texas': 26448193,\n",
    "                   'New York': 19651127,\n",
    "                   'Florida': 19552860,\n",
    "                   'Illinois': 12882135}\n",
    "pop_ds = pd.Series(population_dict)\n",
    "pop_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking the values attribute → underlying <class 'numpy.ndarray'> of the Pandas Series:\n",
      "[38332521 26448193 19651127 19552860 12882135]\n",
      "Shape of the Pandas Series: (5,)\n",
      "Index of the Pandas Series:\n",
      "Index(['California', 'Texas', 'New York', 'Florida', 'Illinois'], dtype='object')\n",
      "As an object, Pandas Series is <class 'pandas.core.series.Series'> type\n",
      "The dtype of each element of the Pandas Series:int64\n"
     ]
    }
   ],
   "source": [
    "# Once created, we can inspect the contents & structure of the Series using values, shape, index and dtype\n",
    "print(f'Invoking the values attribute → underlying {type(pop_ds.values)} of the Pandas Series:\\n{pop_ds.values}')\n",
    "print(f'Shape of the Pandas Series: {pop_ds.shape}')\n",
    "print(f'Index of the Pandas Series:\\n{pop_ds.index}')\n",
    "print(f'As an object, Pandas Series is {type(pop_ds)} type')\n",
    "print(f'The dtype of each element of the Pandas Series:{pop_ds.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Your Own DataFrame\n",
    "\n",
    "- We can build a `Dataframe` using the similarly defined `pd.DataFrame(data)`\n",
    "    - `data` can be a 2-D numpy array, `dict` of lists, `dict` of `Series` objects\n",
    "- Once created, we can invoke `values`, `shape` and `index`\n",
    "    - `dtype` attribute is not applicable to `Dataframe` → we need to use the `dtypes` attribute\n",
    "    - `columns` → Column `Index` associated with the `Dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.059544</td>\n",
       "      <td>1.389960</td>\n",
       "      <td>0.293970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.482753</td>\n",
       "      <td>-0.508393</td>\n",
       "      <td>0.092123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.296962</td>\n",
       "      <td>0.472722</td>\n",
       "      <td>-0.677814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.224266</td>\n",
       "      <td>1.686711</td>\n",
       "      <td>-0.574419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.093211</td>\n",
       "      <td>-0.631989</td>\n",
       "      <td>1.189062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  2.059544  1.389960  0.293970\n",
       "1 -0.482753 -0.508393  0.092123\n",
       "2  0.296962  0.472722 -0.677814\n",
       "3 -0.224266  1.686711 -0.574419\n",
       "4 -0.093211 -0.631989  1.189062"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From a 2D numpy array\n",
    "# Like pd.Series(), pd.DataFrame() has index and columns parameter\n",
    "df_array = pd.DataFrame(np.random.randn(5, 3))\n",
    "df_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2\n",
      "0  2.059544  1.389960  0.293970\n",
      "1 -0.482753 -0.508393  0.092123\n",
      "2  0.296962  0.472722 -0.677814\n",
      "3 -0.224266  1.686711 -0.574419\n",
      "4 -0.093211 -0.631989  1.189062\n"
     ]
    }
   ],
   "source": [
    "# The fancy formatting disappears and readability suffers \n",
    "# when we use the print() function\n",
    "print(df_array)\n",
    "\n",
    "# Beautiful table that you saw earlier → HTML version of df_array\n",
    "# It was rendered automatically by Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.059544</td>\n",
       "      <td>1.389960</td>\n",
       "      <td>0.293970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.482753</td>\n",
       "      <td>-0.508393</td>\n",
       "      <td>0.092123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.296962</td>\n",
       "      <td>0.472722</td>\n",
       "      <td>-0.677814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.224266</td>\n",
       "      <td>1.686711</td>\n",
       "      <td>-0.574419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.093211</td>\n",
       "      <td>-0.631989</td>\n",
       "      <td>1.189062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  2.059544  1.389960  0.293970\n",
       "1 -0.482753 -0.508393  0.092123\n",
       "2  0.296962  0.472722 -0.677814\n",
       "3 -0.224266  1.686711 -0.574419\n",
       "4 -0.093211 -0.631989  1.189062"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# IPython is Interactive Python, which powers these Jupyter Notebooks\n",
    "# It contains a display module that, in turn, contains the display() function\n",
    "# This function extends print() in some ways and is tailored for cell blocks in notebooks\n",
    "from IPython.display import display\n",
    "display(df_array)\n",
    "\n",
    "# The issue with earlier approach for generating beautiful arrays\n",
    "# is that by default, only one beautiful table can be displayed per cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-0.652258</td>\n",
       "      <td>-0.102793</td>\n",
       "      <td>0.847565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>-0.001708</td>\n",
       "      <td>0.439190</td>\n",
       "      <td>0.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>1.069584</td>\n",
       "      <td>-1.731124</td>\n",
       "      <td>0.968662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>-0.592261</td>\n",
       "      <td>-1.051914</td>\n",
       "      <td>-0.908302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0.182826</td>\n",
       "      <td>1.161710</td>\n",
       "      <td>1.302781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       col1      col2      col3\n",
       "a -0.652258 -0.102793  0.847565\n",
       "b -0.001708  0.439190  0.152000\n",
       "c  1.069584 -1.731124  0.968662\n",
       "d -0.592261 -1.051914 -0.908302\n",
       "e  0.182826  1.161710  1.302781"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Like pd.Series(), pd.DataFrame() has index and columns parameter\n",
    "df_array_new = pd.DataFrame(np.random.randn(5, 3),\n",
    "                            index=['a', 'b', 'c', 'd', 'e'],\n",
    "                            columns=['col1', 'col2', 'col3'])\n",
    "display(df_array_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>salaries</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Snow</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tyrion Lannister</td>\n",
       "      <td>25000</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bilbo Baggins</td>\n",
       "      <td>30000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Frodo Baggins</td>\n",
       "      <td>40000</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zaphod Beeblebrox</td>\n",
       "      <td>50000</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               names  salaries  age\n",
       "0          John Snow         0   35\n",
       "1   Tyrion Lannister     25000   40\n",
       "2      Bilbo Baggins     30000   50\n",
       "3      Frodo Baggins     40000   45\n",
       "4  Zaphod Beeblebrox     50000   60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# From a dictionary of lists\n",
    "# Keys are the column names & Values are the column values\n",
    "data_l = {\n",
    "            'names' : ['John Snow', 'Tyrion Lannister', 'Bilbo Baggins', 'Frodo Baggins','Zaphod Beeblebrox'],\n",
    "            'salaries' : [0, 25000, 30000, 40000, 50000],\n",
    "            'age' : [35, 40, 50, 45, 60]\n",
    "        }   \n",
    "\n",
    "df_lists = pd.DataFrame(data_l)\n",
    "display(df_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "California    423967\n",
       "Texas         695662\n",
       "New York      141297\n",
       "Florida       170312\n",
       "Illinois      149995\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From a dictionary of Series objects\n",
    "# Remember how we created a Pandas Series of US state populations?\n",
    "\n",
    "# population_dict = {'California': 38332521, 'Texas': 26448193, 'New York': 19651127, \n",
    "#                    'Florida': 19552860, 'Illinois': 12882135}\n",
    "# pop_ds = pd.Series(population_dict)\n",
    "\n",
    "# Let's combine this with a Pandas Series that stores areas of the same US states\n",
    "area_dict = {'California': 423967, 'Texas': 695662, 'New York': 141297,\n",
    "             'Florida': 170312, 'Illinois': 149995}\n",
    "area_ds = pd.Series(area_dict)\n",
    "area_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>38332521</td>\n",
       "      <td>423967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Texas</th>\n",
       "      <td>26448193</td>\n",
       "      <td>695662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>19651127</td>\n",
       "      <td>141297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Florida</th>\n",
       "      <td>19552860</td>\n",
       "      <td>170312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illinois</th>\n",
       "      <td>12882135</td>\n",
       "      <td>149995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            population    area\n",
       "California    38332521  423967\n",
       "Texas         26448193  695662\n",
       "New York      19651127  141297\n",
       "Florida       19552860  170312\n",
       "Illinois      12882135  149995"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cities = pd.DataFrame({'population': pop_ds, \n",
    "                        'area': area_ds})\n",
    "display(cities)\n",
    "\n",
    "# How many columns does this dataframe have? Give reasons for your answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a special case, you can also convert a `Series` to a `Dataframe` using `pd.DataFrame(series)`\n",
    "- This creates a `Dataframe` with a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the Series with shape (5,) and rank 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "California    423967\n",
       "Texas         695662\n",
       "New York      141297\n",
       "Florida       170312\n",
       "Illinois      149995\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the DataFrame with shape (5, 1) and rank 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>423967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Texas</th>\n",
       "      <td>695662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>141297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Florida</th>\n",
       "      <td>170312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illinois</th>\n",
       "      <td>149995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              area\n",
       "California  423967\n",
       "Texas       695662\n",
       "New York    141297\n",
       "Florida     170312\n",
       "Illinois    149995"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# From a single Series object\n",
    "area_df = pd.DataFrame(area_ds, columns=['area'])\n",
    "\n",
    "# Can you spot the difference?\n",
    "print(f\"Printing the Series with shape {area_ds.shape} and rank {area_ds.values.ndim}\")\n",
    "display(area_ds)\n",
    "print(f\"Printing the DataFrame with shape {area_df.shape} and rank {area_df.values.ndim}\")\n",
    "display(area_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"read\"></a>\n",
    "\n",
    "## Skill #2: Reading CSVs & Excels as Pandas DataFrames\n",
    "\n",
    "- As mentioned earlier → you will mostly be reading in your own txt/csv/excel files into `Series` & `Dataframe`\n",
    "- In Excel, once your file exceeds 10K rows → things start to slow down pretty quickly\n",
    "    - All of us have experienced this → you are in good company!\n",
    "- In fact, Excel caps a single spreadsheet in a workbook to slightly above 1M rows\n",
    "    - At that point, your calculations will take forever to compute. Or force Excel to crash\n",
    "- Pandas has no such limits. Pandas also handles millions of data points seamlessly\n",
    "- `read_csv()` and `read_excel()` are the two functions that we will be using to read in your data\n",
    "- They contain many interesting parameters (eg skipping first rows because of blank space etc)\n",
    "- We cover only the basics here! So, refer to the documentation of both functions to learn more\n",
    "    - Pay attention to `skiprows`, `header`, `usecols`, `index_col` and `sheetnames` to start with\n",
    "- Not just restricted to excel, csvs → can also pull data from relational DBs, HDF5, SPSS, SAS files etc!\n",
    "    - Refer the additional info section for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ssn</th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>country_of_origin</th>\n",
       "      <th>income</th>\n",
       "      <th>dob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>1982-06-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>1971-02-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>1983-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>1968-07-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>1993-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>32556</td>\n",
       "      <td>32557</td>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>1994-03-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>32557</td>\n",
       "      <td>32558</td>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>1981-02-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>32558</td>\n",
       "      <td>32559</td>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>1963-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>32559</td>\n",
       "      <td>32560</td>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>1999-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>32560</td>\n",
       "      <td>32561</td>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>287927</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>1969-11-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0    ssn  age          workclass  fnlwgt    education  \\\n",
       "0               0      1   39          State-gov   77516    Bachelors   \n",
       "1               1      2   50   Self-emp-not-inc   83311    Bachelors   \n",
       "2               2      3   38            Private  215646      HS-grad   \n",
       "3               3      4   53            Private  234721         11th   \n",
       "4               4      5   28            Private  338409    Bachelors   \n",
       "...           ...    ...  ...                ...     ...          ...   \n",
       "32556       32556  32557   27            Private  257302   Assoc-acdm   \n",
       "32557       32557  32558   40            Private  154374      HS-grad   \n",
       "32558       32558  32559   58            Private  151910      HS-grad   \n",
       "32559       32559  32560   22            Private  201490      HS-grad   \n",
       "32560       32560  32561   52       Self-emp-inc  287927      HS-grad   \n",
       "\n",
       "       education-num       marital_status          occupation    relationship  \\\n",
       "0                 13        Never-married        Adm-clerical   Not-in-family   \n",
       "1                 13   Married-civ-spouse     Exec-managerial         Husband   \n",
       "2                  9             Divorced   Handlers-cleaners   Not-in-family   \n",
       "3                  7   Married-civ-spouse   Handlers-cleaners         Husband   \n",
       "4                 13   Married-civ-spouse      Prof-specialty            Wife   \n",
       "...              ...                  ...                 ...             ...   \n",
       "32556             12   Married-civ-spouse        Tech-support            Wife   \n",
       "32557              9   Married-civ-spouse   Machine-op-inspct         Husband   \n",
       "32558              9              Widowed        Adm-clerical       Unmarried   \n",
       "32559              9        Never-married        Adm-clerical       Own-child   \n",
       "32560              9   Married-civ-spouse     Exec-managerial            Wife   \n",
       "\n",
       "         race   gender  capital_gain  capital_loss  hours_per_week  \\\n",
       "0       White     Male          2174             0              40   \n",
       "1       White     Male             0             0              13   \n",
       "2       White     Male             0             0              40   \n",
       "3       Black     Male             0             0              40   \n",
       "4       Black   Female             0             0              40   \n",
       "...       ...      ...           ...           ...             ...   \n",
       "32556   White   Female             0             0              38   \n",
       "32557   White     Male             0             0              40   \n",
       "32558   White   Female             0             0              40   \n",
       "32559   White     Male             0             0              20   \n",
       "32560   White   Female         15024             0              40   \n",
       "\n",
       "      country_of_origin  income         dob  \n",
       "0         United-States   <=50K  1982-06-04  \n",
       "1         United-States   <=50K  1971-02-03  \n",
       "2         United-States   <=50K  1983-09-29  \n",
       "3         United-States   <=50K  1968-07-09  \n",
       "4                  Cuba   <=50K  1993-03-08  \n",
       "...                 ...     ...         ...  \n",
       "32556     United-States   <=50K  1994-03-22  \n",
       "32557     United-States    >50K  1981-02-25  \n",
       "32558     United-States   <=50K  1963-12-02  \n",
       "32559     United-States   <=50K  1999-03-26  \n",
       "32560     United-States    >50K  1969-11-28  \n",
       "\n",
       "[32561 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('data/adult.csv')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24508\\1669701270.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_xls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../levis_datasets/Divyam_Demographic_data.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_xls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\nilm\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\nilm\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         raise ValueError(\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\nilm\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, io, engine)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_stringify_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\nilm\\lib\\site-packages\\pandas\\io\\excel\\_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \"\"\"\n\u001b[0;32m     19\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"xlrd\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\nilm\\lib\\site-packages\\pandas\\compat\\_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, raise_on_missing, on_version)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraise_on_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextra\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd."
     ]
    }
   ],
   "source": [
    "df_xls = pd.read_excel('data/')\n",
    "display(df_xls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24508\\1717648462.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Depending on how large your data is → your computer's memory can quickly fill up\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Good practice to delete them after use and load when required\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mdf_xls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Both Series & DataFrames are Python objects \n",
    "# Depending on how large your data is → your computer's memory can quickly fill up\n",
    "# Good practice to delete them after use and load when required\n",
    "del df\n",
    "del df_xls\n",
    "\n",
    "# Try loading the file\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading UCI Census Income Dataset\n",
    "\n",
    "- Let's load in a reasonably simple and popular dataset to demonstrate what Pandas can do\n",
    "- The Adult Data Set is a popular ML dataset that contains information about the income of different people\n",
    "    - Has over 50K rows and 14 columns (numbers, strings etc)\n",
    "    - Used to train a model that predicts whether an individual earns more than $50K per year\n",
    "    - Available in the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/adult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we learn how to load in data...directly from the web!\n",
    "\n",
    "# UCI datasets are commonly stored as .data files (think of them as ndarrays)\n",
    "# with information about what each column contains, present elsewhere in the page\n",
    "# We have taken that info and stored it in a list here\n",
    "cols = ['age', 'workclass', 'fnlwgt', 'education',\n",
    "        'education-num', 'marital_status', 'occupation', 'relationship', 'race',\n",
    "        'gender', 'capital_gain', 'capital_loss', 'hours_per_week',\n",
    "        'country_of_origin', 'income']\n",
    "\n",
    "# read_csv function call for online csv is same as local csvs\n",
    "# names parameter → accepts a list of column names to use\n",
    "df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', names = cols) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prelim\"></a>\n",
    "\n",
    "## Skill #3: Preliminary Investigation of a Dataset\n",
    "\n",
    "- How do we know whether we have succesfully loaded the dataset into our `DataFrame`?\n",
    "- `DataFrames` come with a variety of attribute & methods that let you investigate structure quickly\n",
    "    - Already seen `dtypes`, `shape`, `index` and `columns`\n",
    "    - `head(n)` & `tail(n)` → first and last `n` rows respectively (`n=5` by default)\n",
    "    - `sample(n)` → randomly samp|les `n` rows from the dataframe\n",
    "    - `info()` → top-level information like # of rows, columns, dtypes, null values, memory usage etc \n",
    "    - `describe()` → a variety of summary statistics like mean, std, min, max, etc for numerical columns\n",
    "- All these methods return a `DataFrame` or `Series` or `Index`\n",
    "    - In addition to displaying the results, you can also use variables to catch the values!    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect structure using `dtypes`, `shape`, `index` & `columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Data types of the Adult Dataset DataFrame: \\n{df.dtypes}')\n",
    "print(f'\\nShape of the Adult Dataset DataFrame: {df.shape}')\n",
    "print(f'\\nRow Index of the Adult Dataset DataFrame: \\n{df.index}')\n",
    "\n",
    "# Demonstrating the use of a variable to captrr\n",
    "\n",
    "print(f'\\nCol Index of the Adult Dataset DataFrame: \\n{df.columns}')\n",
    "\n",
    "# Note that we can see all of these info with a single line of code later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Pandas Datatypes\n",
    "\n",
    "| Pandas Datatype | Python Datatype | Usage |\n",
    "|:--|:--|:--|\n",
    "| `int64` | `int` | Integer numbers |\n",
    "| `object` | `str` | Text, mixture of numeric & non-numeric vals in a column|\n",
    "| `float64` | `float` | Floating point numbers |\n",
    "| `bool` | `bool` | True/False values |\n",
    "\n",
    "There also exists additional data types like `datetime64`, `timedelta64`, `category` etc. We will encounter a few of these data types in later notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quickly glimpse rows using `head`, `tail` and `sample`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the first five rows with:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we wanted more (or less), set n to appropriate value\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversely, we can get the last 5 rows using:\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also get a random sample of n rows\n",
    "df.sample(5)\n",
    "\n",
    "# Note how these are the not the first/last 5, as evident from their index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics using `info()`, `describe()` & `value_counts()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember our earlier comment about that one line of code?\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the numerical statistics for the numerical columns\n",
    "display(df.describe())   # Why do you think we used display() here?\n",
    "\n",
    "# Note that this is just a DataFrame as well!\n",
    "print(type(df.describe()))\n",
    "\n",
    "# Question: What do you think is the index & columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When number of columns is sometimes large, we can also use the transpose for readability\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Exercise: Loading & Inspecting a Dataset\n",
    "\n",
    "- There exists 3 datasets in the folder `filtered_datasets` in `levis_datasets` in the parent directory to where the notebook is\n",
    "- Load the `clustering_mall_customers.csv` dataset, which contains info of different kinds of mall-going customers\n",
    "- Inspect the structure of the dataset using all the attributes & methods we just discussed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"select\"></a>\n",
    "\n",
    "## Skill #4: Selecting single and multiple columns\n",
    "\n",
    "- Very often, we wish to select a column or subset of columns from a `DataFrame`\n",
    "- We can access a single column either as a `Series` or a single-column `DataFrame`\n",
    "    - As a `Series` → using `[]` or `.` operators\n",
    "    - As a `DataFrame` → using only `[]` operator\n",
    "- Multiple columns can only be accessed as a `DataFrame` using `[]` operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can look at a single column like this\n",
    "ds = df[\"workclass\"]\n",
    "display(ds)\n",
    "\n",
    "# If we want Pandas to return a Series, we need to pass the column name into df[...]\n",
    "# 'workclass' is the column name passed inside df[...]\n",
    "\n",
    "# Brief aside, notice how a single column is of dtype series\n",
    "print(f'This is of type {type(df[\"workclass\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also do this to select a column\n",
    "ds = df.workclass\n",
    "display(ds)\n",
    "\n",
    "# We recommend using the previous syntax\n",
    "# Can you think of any issues with this approach? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to view it as a dataframe, you need to use [[]] instead of []\n",
    "display(df[['workclass']])\n",
    "\n",
    "# If we want Pandas to return a dataframe, we need to pass a LIST of column names into df[...]\n",
    "# ['workclass'] is the 1-membered LIST we pass into df[...] \n",
    "\n",
    "# Notice how a single column is of dtype series\n",
    "print(f'This is of type {type(df[\"workclass\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can extend this to multiple columns\n",
    "display(df[[\"age\", \"workclass\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Unique Elements\n",
    "\n",
    "- Often times, we deal with categorical columns, which will have repeat entries\n",
    "    - Think of a column like \"campaign type\" in an email campaign performance report\n",
    "- We might want to extract unique values from that column\n",
    "    - Might also be interested in how many times each unique value occurs in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the unique values for a single series\n",
    "ds.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a description of any categorical columns\n",
    "df[\"workclass\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"indexing\"></a>\n",
    "\n",
    "## Skill #5: Indexing, Selecting and Filtering\n",
    "\n",
    "- Remember how we said `DataFrames` can be thought to be generalizations of `ndarrays`?\n",
    "- We can do all of `ndarray` data access operations with `DataFrame` operations:\n",
    "    - Indexing (e.g. `arr[2, 1]`)\n",
    "    - Slicing (e.g. `arr[:, 1:5]`)\n",
    "    - Masking (e.g. `arr[arr > 0]`)\n",
    "    - Fancy Indexing (e.g. `arr[0, [1, 5]]`)\n",
    "    - Combinations of the above (e.g. `arr[:, [1, 5]]`)\n",
    "- Use special indexer attributes \n",
    "    - `.loc` → label-based indexing (e.g. column with label `'occupation'`)\n",
    "    - `.iloc` → integer position-based indexing (e.g. 5th column) → like an `ndarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(ds.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing operations\n",
    "print(f'3rd element in the series is {ds.loc[2]}\\n')\n",
    "print(f'\"Occupation\" column in df is {df.loc[:, \"occupation\"]}\\n')\n",
    "print(f'5th column in df is {df.iloc[:, 4]}')\n",
    "\n",
    "# Question: Do you think df.loc[2, :] will work? What about df.iloc[2, :]?\n",
    "# Will there be any difference between the two? Why/why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing operations\n",
    "display(ds.loc[:3])\n",
    "display(df.loc[10:13, 'capital_gain':'country_of_origin'])\n",
    "display(df.iloc[2:6, 4:8])\n",
    "\n",
    "# Note how we could use label-based slicing as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since .loc is label-based, it is inclusive of the end index\n",
    "# .iloc is position-based (like lists & ndarrays) and hence, exclusive of end index\n",
    "print('Using loc:')\n",
    "display(df.loc[5:10, :])\n",
    "print('Using iloc:')\n",
    "display(df.iloc[5:10, :])\n",
    "\n",
    "# Notice any difference between the two?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking operations\n",
    "# First, we must build the mask\n",
    "mask = df.age > 30 \n",
    "\n",
    "# You can inspect the mask with:\n",
    "display(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply the mask\n",
    "display(df.loc[mask]) # Same as df.loc[mask, :] or df[mask]\n",
    "\n",
    "# You can also combine the 2 steps using df.loc[df.age > 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like with numpy arrays, you can also get the inverse selection\n",
    "display(df[~mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fancy indexing operations\n",
    "display(df.loc[df.age < 50, ['age', 'workclass']])\n",
    "\n",
    "# Note how we combined boolean indexing along the vertical axis (rows)\n",
    "# and fancy indexing along the horizontal axis (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Is there a difference between df.iloc[4, :] & df.iloc[4:5, :]?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Use boolean indexing to get rows where age is greater than 30 and gender is female\n",
    "# Notice anything strange? :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sorting\"></a>\n",
    "\n",
    "## Skill #6: Sorting Your Data\n",
    "\n",
    "- Most common methods you would use to sort your data\n",
    "    - `sort_values(col_name)` → sorts as per values in the column `col_name`\n",
    "    - `sort_index()` → sorts as per the `Index`\n",
    "        - Might seem trivial with a `RangeIndex`\n",
    "        - Very useful after groupbys, pivot tables, etc\n",
    "- By default, both methods sorts in ascending order\n",
    "    - To sort in descending order, use `ascending=False` parameter\n",
    "- Both methods return a new sorted `DataFrame`\n",
    "    - Default behaviour for most, if not all, pandas methods\n",
    "    - Useful in preventing accidental overwrites but has memory overheads\n",
    "- If you wish to change the original `DataFrame`, use `inplace=True` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's sort the dataframe by the 'age' column\n",
    "display(df.sort_values(\"age\").head())\n",
    "\n",
    "# Notice the \"jumbled up\" Index? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also sort by multiple columns → just pass it as a list\n",
    "# Variables can also be used to catch the returned dataframe object\n",
    "\n",
    "df_age_race_sorted = df.sort_values([\"age\", \"race\"])\n",
    "display(df_age_race_sorted.head())\n",
    "\n",
    "# Order of column names in the list determines the sort order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating the use of sort_index()\n",
    "display(df_age_race_sorted.sort_index().head())\n",
    "\n",
    "# We are back to where we started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It might seem that df_age_race_sorted is altered, but it's not the case\n",
    "display(df_age_race_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inplace=True → orig dataframe is altered, a fresh copy is not returned\n",
    "df_age_race_sorted.sort_index(inplace=True)\n",
    "\n",
    "display(df_age_race_sorted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apart from accidental modification, you will need to exercise caution with `inplace=True`\n",
    "- Parameter ensures that no copies/views of dataframes are returned\n",
    "    - Instead, it returns a `NoneType` Python object\n",
    "        - Python functions that do not **return** values return a `NoneType` \"object\" as placeholder\n",
    "- Why should you bother about this?\n",
    "    - Chaining of other functions (e.g. `head()`) will throw up an error\n",
    "    - However, the original dataframe will be altered!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently df_age_race_sorted has the same order as df as we invoked sort_index(inplace=True)\n",
    "df_age_race_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use sort_values(inplace=True) to sort now, followed by chaining the head()\n",
    "df_age_race_sorted.sort_values(['age', 'gender'], inplace=True).head()\n",
    "\n",
    "# Inspect the error - what does it tell you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But...has df_age_race_sorted...changed in spite of the error?\n",
    "df_age_race_sorted.head()\n",
    "\n",
    "# Tread cautiously!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly, we can also sort as per descending order as well\n",
    "display(df.sort_values(\"age\",ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to sort by multiple columns with different sort orders?\n",
    "display(df.sort_values([\"age\", \"race\"],ascending=[False, True]).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"drop_rename\"></a>\n",
    "\n",
    "## Skill #7: Dropping & Renaming Columns\n",
    "\n",
    "- Ever had to work with spreadsheets that have a lot of columns or weirdly named ones?\n",
    "    - `drop` and `rename` methods are very useful for this\n",
    "- We will briefly look at these 2 methods\n",
    "    - Will be covered in greater detail in _Data Cleaning_ notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load a slightly modified version of the dataset\n",
    "# to demonstrate these functions\n",
    "df = pd.read_csv(\"data/adult.csv\", index_col=0)\n",
    "df.head()\n",
    "\n",
    "# There are 2 new columns in this df —> Can you tell which ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just pass the column name you want to drop\n",
    "df.drop(columns=\"fnlwgt\").head()\n",
    "\n",
    "# Do you think the original df is modified? Why/why not?\n",
    "# Can you think of a way to switch this behaviour?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to drop multiple columns?\n",
    "df.drop(columns=[\"fnlwgt\", \"ssn\"]).head()\n",
    "\n",
    "# How about now? Will the columns still be there in df?\n",
    "# Can you recommend how we can drop these columns from df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop the columns permanently from the dataframe\n",
    "df.drop(columns=[\"fnlwgt\", \"ssn\"], inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping NULL entries\n",
    "\n",
    "- Corrupt or missing data is a common problem for data scientists to deal with\n",
    "- Not so straightforward to understand completely - could be:\n",
    "    - _Missing Completely At Random_ \n",
    "        - E.g., weight missing because scales had faulty batteries\n",
    "        - Regardless of any observable - slightly hypothetical\n",
    "    - _Missing At Random_  \n",
    "        - E.g., faulty scales that produce more missing values when kept on a soft surface\n",
    "        - Whether surface soft/not is an observable\n",
    "        - Easier to find this dependence and model missing values as say, a function of the observable\n",
    "    - _Missing Not At Random_ \n",
    "        - E.g. faulty scales that produce more missing values as time increases, surface smoothness, weight increases etc.\n",
    "        - Might not all be observable. Might also not be able to find all observables that has an impact\n",
    "- Not so straightforward to deal with missing values also\n",
    "    - Ignore them?\n",
    "    - Drop them? \n",
    "    - Replace them with some value? What value?\n",
    "- `dropna()` is used to drop rows & columns with `NaN` values\n",
    "    - Considered in more detail in _Data Cleaning_ notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let us inspect which columns have missing rows\n",
    "display(df.info())\n",
    "\n",
    "# Note the non-null count for each column\n",
    "# More directly, you can also use df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workclass column has NaN values\n",
    "# isna() → generates a mask for rows with NaN values\n",
    "df[df.workclass.isna()]\n",
    "\n",
    "# Want non-NAN values? Invert the mask with ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we want to drop rows where ALL the values are NaN\n",
    "display(df.dropna(how=\"all\").head())\n",
    "\n",
    "print(f'{len(df) - len(df.dropna(how=\"all\"))} rows dropped ')\n",
    "\n",
    "# Do the number of rows dropped, make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we want to drop rows where AT LEAST one entry is NaN\n",
    "display(df.dropna(how=\"any\").head())\n",
    "\n",
    "print(f'{len(df) - len(df.dropna(how=\"any\"))} rows dropped ')\n",
    "\n",
    "# Do the number of rows dropped, make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also specify the column(s) we need to look at\n",
    "df.dropna(how=\"all\",subset=[\"workclass\", 'race']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we want to drop entire columns that have NA values?\n",
    "# i.e., instead of dropping rows with NaNs → we drop columns with NaNs\n",
    "display(df.dropna(how='any', axis=1).head())\n",
    "\n",
    "print(f'{len(df.columns) - len(df.dropna(how=\"any\", axis=1).columns)} cols dropped ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, to make this change permanent, we have to include inplace=True\n",
    "print(f'Dataframe shape before dropping: {df.shape}')\n",
    "\n",
    "df.dropna(how=\"any\",subset=[\"workclass\"],inplace=True)\n",
    "\n",
    "print(f'Dataframe shape after dropping with inplace=True: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename() method requires us to pass a dictionary \n",
    "# keys → old names, values → new_names\n",
    "# Let's rename \"country_of_origin\" to \"nationality\"\n",
    "# and \"dob\" to \"date_of_birth\"\n",
    "\n",
    "renaming_dict = {\"country_of_origin\" : \"nationality\",\n",
    "                 \"dob\" :  \"date_of_birth\"}\n",
    "\n",
    "df.rename(columns=renaming_dict)\n",
    "\n",
    "# Do you think the column names in the orig dataframe have changed? Why/why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like dropna(), if we wish to make the change permanent\n",
    "print(f'Dataframe columns before renaming: \\n{df.columns}')\n",
    "df.rename(columns=renaming_dict, inplace=True)\n",
    "print(f'Dataframe columns after renaming: \\n{df.columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Take some time to work on exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"update\"></a>\n",
    "\n",
    "## Skill #8: Updating Values & Creating New Columns\n",
    "\n",
    "- Creating a new column is easy in Pandas!\n",
    "    - Values can either be constant or dependent on other columns\n",
    "- We demonstrate a couple of different options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what the data looks like\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say we want to create a new column called \"Active\"\n",
    "# with a single value of \"Yes\"\n",
    "df['Active'] = \"Yes\"\n",
    "df.head()\n",
    "\n",
    "# Check the right end of the dataframe. Any new addition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New columns can also depend on other columns\n",
    "df[\"age_in_dog_years\"] = 7 * df.age\n",
    "df[[\"age\",\"age_in_dog_years\"]].head()\n",
    "\n",
    "# Remember how df[[col_1, col_2]] selects only the subset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `np.where()`\n",
    "\n",
    "- Remember how the underlying foundation of Pandas is numpy?\n",
    "    - All numpy functions compatible with `Series` and `Dataframe`\n",
    "- `np.where(...)` is a powerful function for conditional value setting\n",
    "    - Pretty much like `IFS` command in Excel\n",
    "- `np.where(cond, v_true, v_false)` takes in 3 arguments\n",
    "    - `cond` → condition(s) to be checked\n",
    "    - `v_true` → value(s) to be set if condition is true\n",
    "    - `v_false` → value(s) to be set if condition is false\n",
    "- Just like `IFS`, we can have nested conditions\n",
    "    - Let's demonstrate with an example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create an \"Age Group\" column based on \"Age\" column\n",
    "# which takes values of <18 → Minor, 18-60 → Adult, >60 → Senior\n",
    "\n",
    "# We can build this up layer by layer\n",
    "# cond → whether Minor or not, v_true → , v_false → \n",
    "df['age_group'] = np.where(df.age < 18, \"minor\", \"non-minor\")\n",
    "\n",
    "# Let's show a few results\n",
    "print(f'Displaying a few minor row entries:')\n",
    "display(df[df['age'] == 17].head(3))\n",
    "print(f'Displaying a few non-minor row entries:')\n",
    "display(df[df['age'] == 20].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When df.age < 18 is FALSE → age >= 18\n",
    "# We check whether it is < 60 or not and assign the appropriate value\n",
    "df['age_group'] = np.where(df.age < 18, \"minor\", \n",
    "                            np.where(df.age < 60, \"adult\", \"senior\"))\n",
    "\n",
    "# Let's show a few results\n",
    "print(f'Displaying a few minor row entries:')\n",
    "display(df[df['age'] == 17].head(3))\n",
    "print(f'Displaying a few adult row entries:')\n",
    "display(df[df['age'] == 20].head(3))\n",
    "print(f'Displaying a few senior row entries:')\n",
    "display(df[df['age'] > 60].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `apply()`\n",
    "\n",
    "- What if you don't have readymade functions to generate new dependent columns?\n",
    "    - Might be possible to use a really long `np.where()` function call\n",
    "    - Might want to give `apply()` a try!\n",
    "- `apply()` → a method for `Series` and `Dataframe`, which accepts a function as an argument\n",
    "    - Invoking `apply()` for a `Series`? Function must work on a single element (e.g. `int`, `str`)\n",
    "    - Invoking `apply()` for a `Dataframe`? Function must work on a `Series` (e.g. a dataframe column)\n",
    "- `apply()` acts like a loop and executes that function for all constituent components\n",
    "    - For a `Series`, it applies the function to all elements\n",
    "    - For a `Dataframe`, it applies the function along an axis\n",
    "        - `axis=0` → applies along the `Index` → i.e., to each column\n",
    "        - `axis=1` → applies along the `Columns` → i.e. to each row\n",
    "        - This might be confusing, don't worry! Think in terms of top-down & left-right directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we wish to find out who has zero capital_gain or/and capital_loss\n",
    "# Let's define a function gain_loss_id() which accepts a row of data,\n",
    "# Compares values in 2 columns — capital_gain & capital_loss — and \n",
    "# Returns one of 4 strings — \"Zero Loss\", \"Zero Gain\", \"Both Zero\" & \"Both Non-Zero\"\n",
    "\n",
    "def gain_loss_id(your_row):    \n",
    "    \n",
    "    if (your_row['capital_gain'] == 0) & (your_row['capital_loss'] == 0):\n",
    "        return \"Both Zero\"\n",
    "    elif (your_row['capital_gain'] != 0) & (your_row['capital_loss'] == 0):\n",
    "        return \"Only Loss Zero\"\n",
    "    elif (your_row['capital_gain'] == 0) & (your_row['capital_loss'] != 0):\n",
    "        return \"Only Gain Zero\"\n",
    "    else:\n",
    "        return \"Both Non-zero\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the method and inspect the dataframe\n",
    "df[\"gain_loss_id\"] = df.apply(gain_loss_id, axis=1)\n",
    "df.head()\n",
    "\n",
    "# Note the axis argument when apply() is called on dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's demonstrate an apply() method invoked on a Series\n",
    "# Suppose we want to find out whether someone is currently married or not\n",
    "# Needs to be invoked only on the marital_status column\n",
    "\n",
    "# Let's find the unique elements in marital_status column\n",
    "df.marital_status.unique()\n",
    "\n",
    "# Do you notice any issue with the string values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Remove whitespaces\n",
    "df[\"marital_status\"] = df[\"marital_status\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create 3 lists of values that we will compare to\n",
    "no_list = [\"Never-married\", \"Divorced\", \"Separated\", \"Widowed\"]\n",
    "maybe_list = [\"Married-spouse-absent\"]\n",
    "yes_list = ['Married-civ-spouse', 'Married-AF-spouse']\n",
    "\n",
    "# Since we will invoke apply() on Series\n",
    "# Passed function should work on single element (i.e. a string)\n",
    "def check_marital_status(el):    \n",
    "    \n",
    "    if el in yes_list:\n",
    "        return \"Yes\"\n",
    "    elif el in maybe_list:\n",
    "        return \"Maybe\"\n",
    "    else:\n",
    "        return \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the method and inspect the dataframe\n",
    "df[\"is_currently_married\"] = df[\"marital_status\"].apply(check_marital_status)\n",
    "df.head()\n",
    "\n",
    "# Note how there is no axis argument when apply() is called on a Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using lambda functions with apply()\n",
    "\n",
    "- Often times, what you need can be done by simple functions\n",
    "    - So simple that it can be written in a single line\n",
    "- In comes **lambda functions** — inline \"use & dispose\" functions\n",
    "    - Can be used in place of your defined functions in `apply()`\n",
    "- There are 4 parts to a lambda function:\n",
    "    - The `lambda` keyword\n",
    "    - The arguments\n",
    "    - :\n",
    "    - Function body    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we want to get the square of the Age column\n",
    "display(df.age.apply(lambda x: x**2))\n",
    "\n",
    "# This is equivalent to:\n",
    "# def square_age(x):\n",
    "#   return x**2\n",
    "# display(df.age.apply(square_age))\n",
    "\n",
    "# What are some of the benefits of using lambda over explicit function definition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you want to find out whether hours worker per week is < 40 or not\n",
    "df[\"worked_below_avg_hrs\"] = df[\"hours_per_week\"].apply(lambda x: x < 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda expressions can also incorporate simple if-else conditions\n",
    "# function body → value_true if condition else value_false\n",
    "\n",
    "# Let's rework the previous column\n",
    "df[\"worked_below_avg_hrs_new\"] = df[\"hours_per_week\"].apply(lambda x: \"Yes\" if x < 40 else \"No\")\n",
    "\n",
    "# See how this structure looks exactly like list comprehensions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Exercise: Using `apply()` for changing `gender`\n",
    "\n",
    "- You wish to convert the current values in `gender` column to `f` (for female) and `m` (for males) respectively\n",
    "- Try out `apply()` on both `df` and `df['gender']`\n",
    "    - i.e., with `df`, you are invoking `apply()` on a `Dataframe`\n",
    "    - i.e., with `df['gender']`, you are invoking `apply()` on a `Series`\n",
    "- Experiment with both lambda and normal functions to do the trick\n",
    "- Hint: `gender` column values are not 100% what you expect. Use the `unique()` method on the column to see what the issue is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"agg\"></a>\n",
    "\n",
    "## Skill #9: Aggregating Values\n",
    "\n",
    "- All `numpy` aggregation functions have `pandas` counterparts\n",
    "    - `mean()`, `sum()`, `avg()`, `std()`, `median()`, `min()`, `max()` etc.\n",
    "    - Only compatible with numeric columns\n",
    "- Invoked typically as a method for `Series` or `Dataframe`\n",
    "- `axis` parameter determines axis along which the aggregation is applied in `Dataframe`\n",
    "    - `axis=0` → applies along `Index` → e.g., average of values in a column\n",
    "    - `axis=1` → applies along `Columns` → e.g., average of values in a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a series, no need to specify the axis argument\n",
    "print(f'Average number of hours worked: {df.hours_per_week.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what happens when we apply the same logic to the dataframe\n",
    "df.mean(axis=0)\n",
    "\n",
    "# What do you make of the result?\n",
    "# What kind of object do you think this is? What if we change axis to axis=1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Compute the sum, min, max, mean, median and std of \n",
    "# the difference between 'capital_gain' and 'capital_loss' columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level up your analysis with `groupby()`\n",
    "\n",
    "- What if you want to get more refined aggregate values? For e.g.\n",
    "    - SUM of value of goods sold PER country\n",
    "    - AVG of number of purchase orders PER quarter PER country\n",
    "- These are common but powerful use cases for business analytics\n",
    "- `groupby()` is a powerful method to do just this. Broadly, it has 2 main components:\n",
    "    - How to group the data (e.g., which categorical columns should we use to group)\n",
    "    - How to aggregate the data (e.g., which agg function to apply to which column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to find the MEAN of age values PER each educational category\n",
    "display(df.groupby(['education'])['age'].mean())\n",
    "print(f'The returned object is a {type(df.groupby([\"education\"])[\"age\"].mean())}')\n",
    "\n",
    "# Do you see how we changed the quotes around education and age in second line?\n",
    "# Why do you think we did that? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how the returned object is a Series as we were aggregating only across Age column\n",
    "# We can easily convert this to a DataFrame by enclosing 'age' in double square brackets\n",
    "display(df.groupby(['education'])[['age']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note the structure of the basic `groupby()` method call\n",
    "    1. Since it's a method → `df.groupby(...)`\n",
    "    2. Specify column(s) to group by as a _list_ → `df.groupby(['education'])`\n",
    "    3. Specify column(s) to perform the aggregation on → `df.groupby(['education'])['age']`\n",
    "        - If you wish to obtain a dataframe finally, specify double brackets\n",
    "    4. Invoke the aggregation function → `df.groupby(['education'])['age'].mean()`\n",
    "- Till Step 3, we have `Groupby` objects\n",
    "    - Cannot be displayed nicely in the notebook, like `Series` or `Dataframe`\n",
    "    - An agg function must be applied to a `Groupby` object to convert it into a `Series` or `Dataframe` for display    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out what the datatype of the intermediate steps are\n",
    "\n",
    "print(f'Resulting object of Step 1 : df.groupby → {type(df.groupby)}')\n",
    "print(f'Resulting object of Step 2 : df.groupby([\"education\"]) → {type(df.groupby([\"education\"]))}')\n",
    "print(f'Resulting object of Step 3 (series) : df.groupby([\"education\"][\"age\"]) → {type(df.groupby([\"education\"])[\"age\"])}')\n",
    "print(f'Resulting object of Step 3 (df) : df.groupby([\"education\"][[\"age\"]]) → {type(df.groupby([\"education\"])[[\"age\"]])}')\n",
    "print(f'Resulting object of Step 4 : df.groupby([\"education\"][\"age\"].mean()) → {type(df.groupby([\"education\"])[\"age\"].mean())}')\n",
    "\n",
    "# Pay close attention to the square and regular brackets in a groupby call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can inspect the number of groups contained in the Groupby objects\n",
    "print(f'There are {df.groupby([\"education\"]).ngroups} unique groups when grouped by education column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extending `groupby()`\n",
    "\n",
    "- We can group by multiple columns\n",
    "- We can also aggregate multiple columns\n",
    "- We can also perform multiple aggregate functions on these columns\n",
    "- Lastly, we can also perform 1 aggregate function on 1st column, another along the 2nd, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping along 2 columns — Let's look at AVG age PER income PER nationality\n",
    "df.groupby([\"income\", \"nationality\"])[[\"age\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating along 2 columns — Let's look at AVG capital_gain & capital_loss PER nationality\n",
    "df.groupby([\"nationality\"])[[\"capital_gain\", \"capital_loss\"]].mean().sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple aggregation can be done via passing a dictionary\n",
    "# key → column name, val → aggregating function\n",
    "df.groupby([\"nationality\"]).agg({'age': 'mean', 'capital_gain': 'sum'}).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level up your analysis with pivot tables\n",
    "\n",
    "- Pandas also allows you to generate Excel-style pivot tables\n",
    "- Summarize one or more numeric variables based on 2 other categorical variables\n",
    "    - What is also called a _cross-tabulation_\n",
    "- Executed using the `pd.pivot_table()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data=df, \n",
    "                index=['gender'],       # Row labels\n",
    "                columns=['education'], # Column labels\n",
    "                values='capital_gain',  # Numeric attribute that you want a summary statistic of\n",
    "                aggfunc='mean')         # Specific summary statistic that you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge\"></a>\n",
    "\n",
    "## Skill #10: Merging Dataframes\n",
    "\n",
    "- A powerful method to join multiple dataframes together along a common column(s)/index\n",
    "    - We won't go into much detail here or the exercises → will see this in action with the use case\n",
    "- `pd.merge()` functions implements one-to-one, many-to-one & many-to-many joins/merges\n",
    "    - All of these are accessed via the same function call → type of data determines what finally happens\n",
    "- In the very basic call, `pd.merge()` checks for matching column names in the 2 dataframes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of one-to-one merge\n",
    "df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})\n",
    "df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],\n",
    "                    'hire_date': [2004, 2008, 2012, 2014]})\n",
    "df3 = pd.merge(df1, df2)\n",
    "\n",
    "display(df1)\n",
    "display(df2)\n",
    "display(df3)\n",
    "\n",
    "# Notice how pd.merge() automatically merges along employee column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a many to-many join\n",
    "df4 = pd.DataFrame({'group': ['Accounting', 'Accounting',\n",
    "                              'Engineering', 'Engineering', 'HR', 'HR'],\n",
    "                    'skills': ['math', 'spreadsheets', 'coding', 'linux',\n",
    "                               'spreadsheets', 'organization']})\n",
    "display(df1)\n",
    "display(df4)\n",
    "\n",
    "# You can also mention the merge key explicitly using the on parameter\n",
    "print('After merging:')\n",
    "display(pd.merge(df1, df4, on='group'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the merge Key\n",
    "\n",
    "- `pd.merge()` looks for 1 or more matching column names between the two input `df`s, and uses this as key\n",
    "    - What if the column names to match are not the same?\n",
    "- `left_on` and `right_on` comes to the rescue!\n",
    "    - `left_on` → specifies the column name(s) to match in the left `Dataframe`\n",
    "    - `right_on` → specifies the column name(s) to match in the right `Dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'salary': [70000, 80000, 120000, 90000]})\n",
    "\n",
    "display(df1)\n",
    "display(df5)\n",
    "print('After merging:')\n",
    "display(pd.merge(df1, df5, left_on= 'employee', right_on = 'name'))\n",
    "\n",
    "# Often, during merges with different col names, there will be irrelevant columns\n",
    "print('After merging & removing irrelevant entries:')\n",
    "display(pd.merge(df1, df5, left_on= 'employee', right_on = 'name').drop('name', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the type of join\n",
    "\n",
    "- What happens when a value appears in one key column but not the other?\n",
    "    - In the previous demo, what if Sue's information was not present in the 2nd column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa'],\n",
    "                    'salary': [70000, 80000, 120000]})\n",
    "\n",
    "display(df1)\n",
    "display(df6)\n",
    "print('After merging & removing irrelevant entries:')\n",
    "display(pd.merge(df1, df6, left_on= 'employee', right_on = 'name').drop('name', axis=1))\n",
    "\n",
    "# What do you think happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By default, `pd.merge()` performs what is called an _inner join_\n",
    "  - What if we want 'Sue' to be present in the merged dataframe but with say, `NA` where values are not present?\n",
    "- We can modify the kind of join setting the `how` argument in `pd.merge()`\n",
    "  - `how='inner'` → performs an _inner join_ (default behaviour)\n",
    "  - `how='outer'` → performs an _outer join_\n",
    "  - `how='left'` → performs an _left join_\n",
    "  - `how='right'` → performs an _right join_\n",
    "\n",
    "<figure>\n",
    "  <img src='img/pandas_joins.png' style='width:500px'/>\n",
    "  <figcaption>Image from <a href='https://medium.com/swlh/merging-dataframes-with-pandas-pd-merge-7764c7e2d46d'>Medium</a></figcaption>\n",
    "</figure>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the same merge happens but with an outer join\n",
    "print('After outer join:')\n",
    "display(pd.merge(df1, df6, left_on= 'employee', right_on = 'name', how='outer').drop('name', axis=1))\n",
    "\n",
    "# Showing the old dataframe for reference\n",
    "print('After inner join:')\n",
    "display(pd.merge(df1, df6, left_on= 'employee', right_on = 'name', how='inner').drop('name', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying overlapping column names\n",
    "\n",
    "- What happens when you have 2 dataframes with conflicting/same column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'rank': [1, 2, 3, 4]})\n",
    "df8 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'rank': [3, 1, 4, 2]})\n",
    "\n",
    "display(df7)\n",
    "display(df8)\n",
    "display(pd.merge(df7, df8, on=\"name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By default, `pd.merge()` automatically appends a suffix `_x` or `_y` to make the output columns unique\n",
    "- If these defaults are inappropriaty → specify a custom suffix using the `suffixes` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.merge(df7, df8, on=\"name\", suffixes=[\"_L\", \"_R\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "\n",
    "## Skill #11: Saving Files\n",
    "\n",
    "- What good is a dataframe if you cannot export it to excel and CSV files that can be shared with others?\n",
    "- `to_csv()` and `to_excel()` are the 2 functions that do this\n",
    "    - There are more exporting functions that Pandas offers, just like importing functions\n",
    "    - In this notebook, we showcase the `to_csv()` in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's export the merged dataframe of df7 and df8 to a csv file\n",
    "df_new = pd.merge(df7, df8, on=\"name\", suffixes=[\"_L\", \"_R\"])\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We invoke the to_csv() method on the dataframe\n",
    "df_new.to_csv('data/merged_dataframe.csv', index=False)\n",
    "\n",
    "# Setting index=False ensures that the row labels (0, 1, 2, 3) \n",
    "# are not saved as the first column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "b9OZ0sdi4uuW"
   },
   "source": [
    "## Exercise 4\n",
    "\n",
    "Take some time to work on exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "\n",
    "## Summary of Learning Outcomes\n",
    "\n",
    "1. [Numpy Is Not Enough](#problem)\n",
    "    - Discuss how numpy alone cannot be the main workforce for data analysis\n",
    "    - Motivate pandas as an extension of numpy and how it is very relatable to Excel users\n",
    "<br><br>\n",
    "2. [Pandas: An Overview](#overview)\n",
    "    - A top-level exploration of the different functionality that Pandas has to offer\n",
    "<br><br>\n",
    "3. [Introducing Pandas Objects](#objects)\n",
    "    - How Pandas objects can be thought to _enhancements_ of numpy's structured arrays\n",
    "    - Introduce the 3 Pandas objects — `Series`, `Dataframe` and `Index`\n",
    "    - Provide a visual overview of how a `Dataframe` is a collection of `Series` indexed by 2 `Index`-es along row & column axes\n",
    "<br><br>\n",
    "4. [Creating Your Own Series & DataFrames](#create)\n",
    "    - How Pandas objects are closely related to numpy arrays and dictionaries\n",
    "        - Generalizations of numpy arrays\n",
    "        - Specialization of dictionaries\n",
    "    - Using `pd.Series(...)` to create your own Pandas Series from scratch\n",
    "    - Using `pd.DataFrame(...)` to create your own Pandas Dataframes from scratch\n",
    "    - Investigating the structure of your created Pandas objects using `dtypes`, `shape`, `index` and `columns`\n",
    "<br><br>\n",
    "5. [Reading CSVs & Excels as Pandas DataFrames](#read)\n",
    "    - Motivate the use of Pandas for your really large excel spreadsheets that can crash Excel\n",
    "    - Using `pd.read_csv()` to read CSV files and `pd.read_excel()` to read Excel files\n",
    "    - Mention parameters of these functions like `skiprows`, `header`, `usecols`, `index_col` and `sheetnames` to tweak further behaviour\n",
    "    - Loading the UCI Census Income Dataset from the web\n",
    "<br><br>\n",
    "6. [Preliminary Investigation of a Dataset](#prelim)\n",
    "    - Using `head()`, `tail()` and `sample()` to quickly glimpse at a certain rows\n",
    "    - Using `info()`, `describe()` and `value_counts()` to get a quick top-level view of aggregate statistics\n",
    "    - Introduce common datatypes that Pandas offers\n",
    "<br><br>\n",
    "7. [Selecting single and multiple columns](#select)\n",
    "    - Accessing a single column using `[]` and `.` as a Series and a subset of columns using `[[...]]`as a Dataframe\n",
    "    - Converting a series selection into a dataframe selection using `[[...]]`\n",
    "    - Using `.unique()` to get a list of unique elements\n",
    "<br><br>\n",
    "8. [Indexing, Selecting and Filtering](#indexing)\n",
    "    - Perform the techniques introduced in the numpy notebook → indexing, slicing, masking, fancy indexing and combinations\n",
    "    - Using `.loc` and `.iloc` to select rows and columns\n",
    "<br><br>\n",
    "9. [Sorting Your Data](#sorting)\n",
    "    - Using `.sort_values()` to sort your data\n",
    "    - Using `.sort_index()` to sort your data by index\n",
    "    - Sort data along multiple columns by passing a list of column names\n",
    "    - Tweaking the `ascending` parameter to sort your data in ascending or descending order\n",
    "    - Using `inplace` argument to modify original dataframes, in place\n",
    "        - Mention caveats of how this returns a `NoneType` object and hence, one must exercise caution when chaining the result directly with other functions\n",
    "<br><br>\n",
    "10. [Dropping & Renaming Columns](#drop_rename)\n",
    "    - Using `.drop()` to drop irrelevant columns\n",
    "    - Using `.rename()` to rename weirdly named columns\n",
    "    - Learn the 3 different types of null entries in your data\n",
    "    - Using `.isna()` to generate masks of missing values\n",
    "    - Introducing `.dropna()` to drop rows or columns with missing values\n",
    "    - Tweaking the `how` parameter to `all` or `any` to modify the `dropna()` behaviour\n",
    "    - Setting the `subset` parameter of `dropna()` to check/drop only certain columns\n",
    "    - Using `axis` parameter to drop rows/columns\n",
    "<br><br>\n",
    "11. [Updating Values & Creating New Columns](#update)\n",
    "    - Creating a new column\n",
    "        - With constant values\n",
    "        - Dependent on values in another column\n",
    "        - Using `np.where()`\n",
    "        - Using `.apply()`\n",
    "    - Invoking `.apply()` on a Series and a Dataframe\n",
    "        - How `axis` parameter needs to be specified if invoked on a dataframe\n",
    "    - Using `lambda` and normal functions inside the `.apply()` method\n",
    "<br><br>\n",
    "12. [Aggregating Values](#agg)\n",
    "    - Perform the techniques introduced in the numpy notebook → mean(), sum(), std(), min(), max(), median(), etc.\n",
    "    - Using `axis` parameter to specify the axis along which the aggregation is performed\n",
    "    - Using `.groupby()` to perform more powerful and nuanced aggregation on partitions that you are interested in\n",
    "    - Extending the basic `groupby()` such that you group by multiple columns, aggregate multiple columns and perform multiple aggregating functions\n",
    "    - Using `pd.pivot_table()` function to generate Excel-style pivot tables\n",
    "<br><br>\n",
    "13. [Merging Dataframes](#merge)\n",
    "    - Implement one-to-one, many-to-one and many-to-many joins using `pd.merge()`\n",
    "    - Using `on`, `left_on` and `right_on` parameters to specify the column names to join on\n",
    "    - Using `how` parameter to specify the type of join\n",
    "    - Using `suffixes` parameter to specify the suffixes to use for the new columns\n",
    "<br><br>\n",
    "14. [Saving Files](#save)\n",
    "    - Using `to_csv()` and `to_excel()` to save your dataframes to CSV and Excel files\n",
    "    - Using `index` parameter to specify whether to save the index or not\n",
    "<br><br>\n",
    "\n",
    "<figure>\n",
    "  <img src='img/pd_final.jpg' style='width:500px'/>\n",
    "</figure> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information\n",
    "\n",
    "1. [Pandas Cheatsheet](https://attachments.convertkitcdnn2.com/682688/4d4172ec-3491-43ec-9ce0-f03cc7f0eb1e/Python%20Cheat%20Sheet%20for%20Excel%20Users.pdf) is a comprehensive cheatsheet for those who are making the move from Excel to Pandas via Python. Essentially for all of you! Here is another [beginner-friendly cheatsheet](https://www.dataquest.io/blog/pandas-cheat-sheet/)\n",
    "\n",
    "2. For more comparisons with spreadsheets, check out the [Official Pandas to Spreadsheet Comparisons](https://pandas.pydata.org/pandas-docs/dev/getting_started/comparison/comparison_with_spreadsheets.html) and [A Complete Yet Simple Guide to Move from Excel to Python](https://towardsdatascience.com/a-complete-yet-simple-guide-to-move-from-excel-to-python-d664e5683039)\n",
    "\n",
    "3. Want to get started quickly with Pandas? Check out the official, awesome yet short [10-min guide to Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html)\n",
    "\n",
    "4. Do you want to have a go at learning Pandas once again? Try out [Kevin Markham's excellent YT playlist on Data Analysis with Pandas](https://www.youtube.com/playlist?list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y). Note that we will be covering more topics, specifically about cleaning and feature engineering in subsequent notebooks.\n",
    "\n",
    "5. Want to level up with some best practices? Check out [Kevin Markham's Best practices with Pandas](https://www.youtube.com/playlist?list=PL5-da3qGB5IBITZj_dYSFqnd_15JgqwA6)\n",
    "\n",
    "6. [Intro to pandas data structures](http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/) gives you a more exploratory sneak peek into the different file formats that you can load into Pandas (SQL, HDF5, SAS, Stata etc.)\n",
    "\n",
    "7. [Official Groupby Documentation](https://pandas.pydata.org/docs/reference/groupby.html) provides a clear overview of the different attributes and methods that we can invoke on the groupby object\n",
    "\n",
    "8. Want to learn more about merging and concatenating, joining and appending? Check out the [Official Merging Documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Intro-to-Pandas.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
